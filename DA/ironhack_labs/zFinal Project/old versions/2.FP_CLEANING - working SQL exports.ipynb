{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************\n",
    "# LIBRARIES ****************************************************************************\n",
    "# **************************************************************************************\n",
    "\n",
    "# General-purpose libraries for operating system interactions, JSON data manipulation, and date/time handling.\n",
    "import os  # Interact with the operating system\n",
    "import json  # Work with JSON data\n",
    "from random import randint  # Generate random integers\n",
    "from time import sleep  # Introduce delays in code execution\n",
    "\n",
    "from datetime import datetime, timedelta  # Manipulate dates and times\n",
    "# **************************************************************************************\n",
    "\n",
    "# Fundamental scientific computing libraries like NumPy and Pandas.\n",
    "import numpy as np  # Fundamental package for scientific computing\n",
    "import pandas as pd  # Data manipulation library\n",
    "# **************************************************************************************\n",
    "\n",
    "# **************************************************************************************\n",
    "# **************************************************************************************\n",
    "\n",
    "# Specialized libraries for scientific computing, namely SciPy\n",
    "import scipy  # Scientific computing and technical computing library\n",
    "import scipy.stats as st\n",
    "# **************************************************************************************\n",
    "\n",
    "# Web-related tasks, such as making HTTP requests, parsing HTML/XML data, and web scraping\n",
    "import requests  # Perform HTTP requests\n",
    "from bs4 import BeautifulSoup  # Parse HTML and XML data\n",
    "# **************************************************************************************\n",
    "\n",
    "#String operations\n",
    "import regex as re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Pretty-print Python data structures\n",
    "import pprint\n",
    "# **************************************************************************************\n",
    "\n",
    "# Visualization oriented\n",
    "import matplotlib.pyplot as plt  # Create static visualizations\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "from plotly import express as px  # Create interactive plots and charts\n",
    "\n",
    "# **************************************************************************************\n",
    "# **************************************************************************************\n",
    "\n",
    "# Machine learning : preprocessing, dimensionality reduction, one-hot encoding, and clustering\n",
    "import sklearn  # Machine learning library\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # Standardize features\n",
    "from sklearn.preprocessing import OneHotEncoder  # One-hot encode categorical features\n",
    "\n",
    "from sklearn.decomposition import PCA  # Perform dimensionality reduction\n",
    "from sklearn.cluster import KMeans  # Perform clustering\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    precision_recall_curve, roc_curve, roc_auc_score\n",
    ")\n",
    "\n",
    "\n",
    "#Handling Imbalanced Datasets\n",
    "import imblearn  # Handle imbalanced datasets in machine learning\n",
    "\n",
    "# **************************************************************************************\n",
    "# **************************************************************************************\n",
    "\n",
    "#Web Scraping\n",
    "import scrapy  # Web scraping framework\n",
    "\n",
    "#Saving Stuff\n",
    "import pickle\n",
    "\n",
    "#Deep Learning\n",
    "# import pytorch  # Deep learning library\n",
    "\n",
    "# **************************************************************************************\n",
    "# FUNCTIONS ****************************************************************************\n",
    "# **************************************************************************************\n",
    "\n",
    "#from functions import scoring (name of function)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST LOOK AT THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"steam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"developer\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.loc[df.duplicated(keep=False), :]\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"No duplicate found.\")\n",
    "else:\n",
    "    print(\"The following duplicated have been found:\\n\")\n",
    "    print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick column distribution visualization\n",
    "\n",
    "sns.distplot(df['price'], bins = 1)\n",
    "plt.title('Distribution of column')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fill missing values with a specific value\n",
    "# df['column_name'].fillna(value, inplace=True)\n",
    "\n",
    "# # Fill missing values with the mean of the column\n",
    "# df['column_name'].fillna(df['column_name'].mean(), inplace=True)\n",
    "\n",
    "# # # Drop rows with missing values\n",
    "# # df = df.dropna()\n",
    "\n",
    "# # # Fill missing values with a specific value\n",
    "# # df = df.fillna(value)\n",
    "\n",
    "# # # Forward fill missing values\n",
    "# # df = df.ffill()\n",
    "\n",
    "# # # Backward fill missing values\n",
    "# # df = df.bfill()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop duplicated rows\n",
    "# df = df.drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With n.largest()\n",
    "# display(df[\"column\"].nlargest(5))\n",
    "# display(df[\"column1\"].nlargest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'df' is our DataFrame and 'column' is the column with outliers\n",
    "# mean = np.mean(df['column'])\n",
    "# std = np.std(df['column'])\n",
    "# threshold = 3\n",
    "\n",
    "# df = df[(np.abs(df['column'] - mean) < threshold * std)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns We Dislike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is our DataFrame and 'column' is the unwanted column we want to drop\n",
    "df = df.drop([\"steamspy_tags\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Columns to Reorder Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_columns_order = [\"column\", \"column3\", \"column1\", \"column2\"]\n",
    "# df = df[new_columns_order]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting 1 column into 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split neighbourhood_full\n",
    "# df = df['neighbourhood_full'].str.split(\",\", expand = True)\n",
    "# df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Categorical Values in Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_list = ['Snow', 'Fog', 'Rain', 'Thunderstorm']\n",
    "# df_fixed = df\n",
    "\n",
    "# df_fixed[event_list] = 0\n",
    "\n",
    "# df_fixed['Rain'] = df_fixed['Events'].str.contains('Rain').astype(int)\n",
    "# df_fixed['Snow'] = df_fixed['Events'].str.contains('Snow').astype(int)\n",
    "# df_fixed['Fog'] = df_fixed['Events'].str.contains('Fog').astype(int)\n",
    "# df_fixed['Thunderstorm'] = df_fixed['Events'].str.contains('Thunderstorm').astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns = {'column1': 'column_1', 'column2.0': 'column_2'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rows Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Rows Based on Conditions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FILTERING : Assuming 'df' is our DataFrame and 'column' is the column to filter on\n",
    "# df = df[df['column'] != condition]\n",
    "\n",
    "# # DROPPING ROWS WITH MISSING VALUES\n",
    "# df = df.dropna(subset=['column'])\n",
    "# df = df.dropna(subset=['column1'])\n",
    "\n",
    "# df.isnull().sum()\n",
    "\n",
    "# # DELETING ROWS THAT ARE UNEXPLOITABLE\n",
    "# print(len(df))\n",
    "\n",
    "# # COUNTRIES WHICH ARE NOT\n",
    "# indices_to_drop1 = df.loc[df['location'] == 'Tel Aviv, Israel'].index\n",
    "# df = df.drop(indices_to_drop1)\n",
    "# print(len(df))\n",
    "\n",
    "# # YEARS OF EXPERIENCE = 0\n",
    "# indices_to_drop2 = df.loc[df['years_of_experience'] == 0].index\n",
    "# df = df.drop(indices_to_drop2)\n",
    "# print(len(df))\n",
    "\n",
    "# # MISGENDERING\n",
    "# indices_to_drop3 = df.loc[df['gender'] == \"Title: Senior Software Engineer\"].index\n",
    "# df = df.drop(indices_to_drop3)\n",
    "# print(len(df))\n",
    "\n",
    "# # SPECIFIC ROWS OPERATIONS INSIDE A COLUMN USING GROUPBY\n",
    "# df.groupby('company_name')['title'].value_counts().to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical and Formatting Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Columns Variables to DateTime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting columns to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick column distribution visualization\n",
    "\n",
    "sns.distplot(df['release_date'], bins = 1)\n",
    "plt.title('Distribution of column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Categorical Variables to Numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'df' is our DataFrame and 'column' is the categorical column\n",
    "# df['column'] = df['column'].astype('category')\n",
    "# df['column'] = df['column'].cat.codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Data Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'df' is our DataFrame and 'column' is the column to convert\n",
    "# df['column'] = df['column'].astype(new_data_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['column', \"column_1\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'df' is our DataFrame and 'column' is the column to clean\n",
    "# df['column'] = df['column'].str.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Special Characters from Strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'df' is our DataFrame and 'column' is the column to clean\n",
    "# df['column'] = df['column'].apply(lambda x: re.sub('[^a-zA-Z0-9\\s]', '', x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Capitalized Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['room_type'] = df['room_type'].str.lower()\n",
    "\n",
    "# # df['room_type'] = df['room_type'].str.Capitalize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['column_name'] = df['column_name'].str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = set(stopwords.words('english'))\n",
    "# df['column_name'] = df['column_name'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MANY VALUES: Replace values to 'Shared room', 'Entire place', 'Private room' and 'Hotel room' if applicable.\n",
    "# mappings = {'private room': 'Private Room', \n",
    "#             'private': 'Private Room',\n",
    "#             'entire home/apt': 'Entire place',\n",
    "#             'shared room': 'Shared room',\n",
    "#             'home': 'Entire place'}\n",
    "\n",
    "# # Replace values and collapse data\n",
    "# airbnb['room_type'] = airbnb['room_type'].replace(mappings)\n",
    "# airbnb['room_type'].unique()\n",
    "\n",
    "\n",
    "# # SINGLE VALUE: From \"SOFT ENGINEER\" to \"SOFTWARE ENGINEER\"\n",
    "\n",
    "# print(df['title'].value_counts())\n",
    "# df['title'] = df['title'].replace('Soft Engineer', 'Software Engineer')\n",
    "\n",
    "# print(df['title'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot Visualizations + n.largest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['appid', 'required_age', 'achievements', 'positive_ratings', 'negative_ratings', 'average_playtime', 'median_playtime', 'price']\n",
    "\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.boxplot(df[col].dropna())  # Nous devons Ã©liminer les valeurs manquantes pour que boxplot fonctionne\n",
    "    plt.title(f'Boxplot de {col}')\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel(col)\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['achievements', 'positive_ratings', 'negative_ratings', 'average_playtime', 'median_playtime', 'price']\n",
    "\n",
    "for col in num_cols:\n",
    "    print(f\"10 plus grandes valeurs de {col} avec index :\")\n",
    "    print(df.sort_values(col, ascending=False)[['name', col]].head(10))  # Supposons que 'name' est la colonne avec le nom du jeu\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['required_age', 'achievements', 'positive_ratings', 'negative_ratings', 'average_playtime', 'median_playtime', 'price']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CORRELATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPORTING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"new_steam_data.csv\", sep = \";\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TABLE 6 + 7 with Andy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Game Genres\n",
    "game_genres_df = df[['appid', 'genres']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_genres_df[\"genres\"] = game_genres_df[\"genres\"].str.split(\";\")\n",
    "\n",
    "game_genres_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_game_genres = game_genres_df.explode(\"genres\")\n",
    "exploded_game_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create what will be the dimension table - we take a set of the ingredients so we only have unique values and enumerate it to create ID values\n",
    "genres_table = pd.DataFrame(columns = ['genre_id','genre'])\n",
    "for id, genre in enumerate(set(exploded_game_genres['genres'])):\n",
    "    genres_table.loc[id] = {'genre_id':id, 'genre':genre}\n",
    "\n",
    "genres_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_game_genres_table = exploded_game_genres.merge(genres_table, left_on='genres', right_on='genre')\n",
    "\n",
    "exploded_game_genres_table = exploded_game_genres_table[['appid','genre_id']]\n",
    "exploded_game_genres_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating relevant dataframes to export as SQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 0: Whole DataFrame\n",
    "\n",
    "import getpass\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "sql_pass = getpass.getpass()\n",
    "\n",
    "connection_string = 'mysql+pymysql://root:t3oJbpp38P99T3Jd7cRS@localhost:3306/'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "df.to_sql('whole_steam_store', engine, 'steam store', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Game Info\n",
    "game_info_df = df[['appid', 'name', 'release_date', 'developer', 'publisher']].copy()\n",
    "# Table 2: Game Genres\n",
    "game_genres_df = df[['appid', 'genres']].copy()\n",
    "# Table 3: Game Categoriesgame_genres_df\n",
    "game_categories_df = df[['appid', 'categories']].copy()\n",
    "# Table 4: Game Ratings\n",
    "game_ratings_df = df[['appid', 'positive_ratings', 'negative_ratings']].copy()\n",
    "# Table 5: Game Pricing\n",
    "game_pricing_df = df[['appid', 'price']].copy()\n",
    "# Table 6: Exploded Game genres\n",
    "game_genres_exploded_df = exploded_game_genres_table[['appid', 'genre_id']].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting relevant dataframes to SQL as Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Game Info\n",
    "game_info_df.to_sql('game_info', engine, 'steam store', if_exists='replace', index=False)\n",
    "# Table 2: Game Genres\n",
    "game_genres_df.to_sql('game_genres', engine, 'steam store', if_exists='replace', index=False)\n",
    "# Table 3: Game Categories\n",
    "game_categories_df.to_sql('game_categories', engine, 'steam store', if_exists='replace', index=False)\n",
    "# Table 4: Game Ratings\n",
    "game_ratings_df.to_sql('game_ratings', engine, 'steam store', if_exists='replace', index=False)\n",
    "# Table 5: Game Pricing\n",
    "game_pricing_df.to_sql('game_pricing', engine, 'steam store', if_exists='replace', index=False)\n",
    "# Table 6: Exploded Game genres\n",
    "game_genres_exploded_df.to_sql('game_genres_exploded', engine, 'steam store', if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOVE WORKS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
